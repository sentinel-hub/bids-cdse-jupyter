{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deforestation Monitoring using Sentinel 2 and xarray\n",
    "\n",
    "Sentinel 2 data is some of the most popular satellite data, but it does come with challenges. Cloud free mosaicks have to be constructed often to get analysis ready data, accessing a lot of data through tiles takes a long time and getting the data into a format where it can be easily analysed using common Python tools can be a challenge.\n",
    "\n",
    "In this notebook we will show how this whole process of getting analysis ready data into Python can be sped up by using the Copernicus Dataspace Ecosystem and Sentinel Hub APIs. This is being presented by running through a basic deforestation monitoring use-case. \n",
    "\n",
    "What we show in this notebook:\n",
    "\n",
    "- How to access Sentinel 2 data in the Copernicus Dataspace Ecosystem\n",
    "- Calculation of NDVI in the Cloud\n",
    "- Monthly composites\n",
    "- Creating a time series\n",
    "- Loading data into xarray\n",
    "- Basic classification using thresholding\n",
    "- Accuracy assessment of classification\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- [A Copernicus Dataspcae Ecosystem account](https://documentation.dataspace.copernicus.eu/Registration.html)\n",
    "- Basic understanding of the Sentinel Hub Processing API ([Introductory Notebook available here](https://documentation.dataspace.copernicus.eu/notebook-samples/sentinelhub/data_download_process_request.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a conda package in the current Jupyter kernel\n",
    "import sys\n",
    "!conda install --yes --prefix {sys.prefix} xarray, rioxarray, dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from ipyleaflet import GeoJSON, Map, basemaps\n",
    "from sentinelhub import (\n",
    "    CRS,\n",
    "    BBox,\n",
    "    DataCollection,\n",
    "    MimeType,\n",
    "    SentinelHubDownloadClient,\n",
    "    SentinelHubRequest,\n",
    "    SHConfig,\n",
    ")\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credentials\n",
    "\n",
    "To obtain your `client_id` & `client_secret` you need to navigate to your [Dashboard](https://shapps.dataspace.copernicus.eu/dashboard/#/). In the User Settings you can create a new OAuth Client to generate these credentials. For more detailed instructions, visit the relevent [documentation page](https://documentation.dataspace.copernicus.eu/APIs/SentinelHub/Overview/Authentication.html).\n",
    "\n",
    "Now that you have your `client_id` & `client_secret`, it is recommended to configure a new profile in your Sentinel Hub Python package. Instructions on how to configure your Sentinel Hub Python package can be found [here](https://sentinelhub-py.readthedocs.io/en/latest/configure.html). Using these instructions you can create a profile specific to using the package for accessing Copernicus Data Space Ecosystem data collections. This is useful as changes to the the config class are usually only temporary in your notebook and by saving the configuration to your profile you won't need to generate new credentials or overwrite/change the default profile each time you rerun or write a new Jupyter Notebook. \n",
    "\n",
    "If you are a first time user of the Sentinel Hub Python package for Copernicus Data Space Ecosystem, you should create a profile specific to the Copernicus Data Space Ecosystem. You can do this in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this cell if you have not created a configuration.\n",
    "\n",
    "config = SHConfig()\n",
    "config.sh_client_id = getpass.getpass(\"Enter your SentinelHub client id\")\n",
    "config.sh_client_secret = getpass.getpass(\"Enter your SentinelHub client secret\")\n",
    "config.sh_token_url = \"https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token\"\n",
    "config.sh_base_url = \"https://sh.dataspace.copernicus.eu\"\n",
    "config.save(\"cdse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if you have already configured a profile in Sentinel Hub Python for the Copernicus Data Space Ecosystem, then you can run the below cell entering the profile name as a string replacing `profile_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SHConfig(\"<profile_name>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Area of Interest\n",
    "\n",
    "First we define an area of interest. In this case the area of interest is in the Harz Mountains in Germany since we know that there has been a lot of forest dieback in recent years.\n",
    "\n",
    "The resolution is defined in the units of the coordinate reference system. Because we want to define units in meters, we also need to define the bounding box coordinates in a CRS using meters. We use EPSG:3035 in this case. This CRS is only available for Europe, outside of Europe we could use EPSG:3857 or UTM Zones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desired resolution of our data\n",
    "resolution = (100, 100)\n",
    "bbox_coords = [10.633501, 51.611195, 10.787234, 51.698098]\n",
    "epsg = 3035\n",
    "# Convert to 3035 to get crs with meters as units\n",
    "bbox = BBox(bbox_coords, CRS(4326)).transform(epsg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = bbox.transform(4326).middle\n",
    "\n",
    "# Add OSM background\n",
    "overview_map = Map(basemap=basemaps.OpenStreetMap.Mapnik, center=(y, x), zoom=10)\n",
    "\n",
    "# Add geojson data\n",
    "geo_json = GeoJSON(data=bbox.transform(4326).geojson)\n",
    "overview_map.add_layer(geo_json)\n",
    "\n",
    "# Display\n",
    "overview_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Access\n",
    "\n",
    "Next we define our [evalscript](https://documentation.dataspace.copernicus.eu/APIs/SentinelHub/Evalscript.html). The evalscript is some javascript code which tells the Copernicus Dataspace Ecosystem what to do with the pixels you have requested before they are delivered to you.\n",
    "\n",
    "This makes it a very powerful tool to carry out pixel based calculations in the cloud. For inspiration of what can be done in an evalscript, there is a rich online resource collecting evalscripts made by the community called [custom-scripts](https://custom-scripts.sentinel-hub.com/). In this case we want to calculate cloud free mosaics. This is a perfect application to do in the evalscript since you do not have to download a lot of data to calculate the mosaic yourself, instead all the calculations are done on the server and only the finished cloud free mosaic is delivered. \n",
    "\n",
    "So let's go over how this is done.\n",
    "\n",
    "The evalscript has to define two functions `setup()` and `evaluatePixel()`. First let's look at the setup function:\n",
    "\n",
    "```js\n",
    "function setup() {\n",
    "    return {\n",
    "        input: [\"B08\", \"B04\", \"B03\", \"B02\", \"SCL\"],\n",
    "        output: {\n",
    "            bands: 5,\n",
    "            sampleType: \"INT16\"\n",
    "        },\n",
    "        mosaicking: \"ORBIT\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "Here we define which bands we want to request. In this case we get bands to calculate the NDVI and to display a True Color Image. We also define how our output should be structured, here we want to get a 5 band image with the data type INT16. \n",
    "\n",
    "Finally we specify the mosaicking. This specifies how the pixel values are returned to us. `mosaicking: \"SIMPLE\"` will return only a single pixel, either from the most recent, least recent or least cloudy Sentinel 2 tile.\n",
    "\n",
    "`mosaicking: \"ORBIT\"` on the other hand will return all pixels of unique orbits for the entire time series as a list. This is what we are using to get all possible values to construct the cloud free mosaic from.\n",
    "\n",
    "Next we'll have a look at the `evaluatePixel()` function. This is the function where the actual calculation is defined:\n",
    "\n",
    "```js\n",
    "function evaluatePixel(samples) {\n",
    "    var valid = samples.filter(validate);\n",
    "    if (valid.length > 0 ) {\n",
    "        let cloudless = {\n",
    "            b08: getFirstQuartileValue(valid.map(s => s.B08)),\n",
    "            b04: getFirstQuartileValue(valid.map(s => s.B04)),\n",
    "            b03: getFirstQuartileValue(valid.map(s => s.B03)),\n",
    "            b02: getFirstQuartileValue(valid.map(s => s.B02)),\n",
    "        }\n",
    "        let ndvi = ((cloudless.b08 - cloudless.b04) / (cloudless.b08 + cloudless.b04))\n",
    "        // This applies a scale factor so the data can be saved as an int\n",
    "        let scale = [cloudless.b04, cloudless.b03, cloudless.b02, ndvi].map(v => v*10000);\n",
    "        return scale\n",
    "    }\n",
    "    // If there isn't enough data, retun NODATA\n",
    "    return [-32768, -32768, -32768, -32768]\n",
    "}\n",
    "```\n",
    "\n",
    "The way we construct the cloud free mosaic is by first filtering all the available acquisitions to only includes ones which contain clear data with `samples.filter(validate);`. Then we sort the array and get the value at the first quartile of the array. Getting the first quartile instead of the mean or median further reduces the risk that we select a cloudy pixel.\n",
    "\n",
    "Finally we calculate the NDVI using the cloudless values and return all of our desired values as an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalscript_cloudless = \"\"\"\n",
    "//VERSION=3\n",
    "function setup() {\n",
    "    return {\n",
    "        input: [\"B08\", \"B04\", \"B03\", \"B02\", \"SCL\"],\n",
    "        output: {\n",
    "            bands: 4,\n",
    "            sampleType: \"INT16\"\n",
    "        },\n",
    "        mosaicking: \"ORBIT\"\n",
    "    }\n",
    "}\n",
    "\n",
    "function getFirstQuartileValue(values) {\n",
    "    values.sort((a,b) => a-b);\n",
    "    return getFirstQuartile(values);\n",
    "}\n",
    "\n",
    "function getFirstQuartile(sortedValues) {\n",
    "    var index = Math.floor(sortedValues.length / 4);\n",
    "    return sortedValues[index];\n",
    "}\n",
    "\n",
    "function validate(sample) {\n",
    "    // Define codes as invalid:\n",
    "    const invalid = [\n",
    "        0, // NO_DATA\n",
    "        1, // SATURATED_DEFECTIVE\n",
    "        3, // CLOUD_SHADOW\n",
    "        7, // CLOUD_LOW_PROBA\n",
    "        8, // CLOUD_MEDIUM_PROBA\n",
    "        9, // CLOUD_HIGH_PROBA\n",
    "        10 // THIN_CIRRUS\n",
    "    ]\n",
    "    return !invalid.includes(sample.SCL)\n",
    "}\n",
    "\n",
    "function evaluatePixel(samples) {\n",
    "    var valid = samples.filter(validate);\n",
    "    if (valid.length > 0 ) {\n",
    "        let cloudless = {\n",
    "            b08: getFirstQuartileValue(valid.map(s => s.B08)),\n",
    "            b04: getFirstQuartileValue(valid.map(s => s.B04)),\n",
    "            b03: getFirstQuartileValue(valid.map(s => s.B03)),\n",
    "            b02: getFirstQuartileValue(valid.map(s => s.B02)),\n",
    "        }\n",
    "        let ndvi = ((cloudless.b08 - cloudless.b04) / (cloudless.b08 + cloudless.b04))\n",
    "        // This applies a scale factor so the data can be saved as an int\n",
    "        let scale = [cloudless.b04, cloudless.b03, cloudless.b02, ndvi].map(v => v*10000);\n",
    "        return scale\n",
    "    }\n",
    "    // If there isn't enough data, return NODATA\n",
    "    return [-32768, -32768, -32768, -32768]\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have defined how the pixels should be handled. However we still need to define some other parameters to get a full request. \n",
    "\n",
    "We need to define which data we want to use and the time-frame of the data.\n",
    "\n",
    "This is what we are doing in the next cell. Here we also start building our time series. To see changes over the years we want to get cloud mosaics for the same 3 months over the years. To do that we are defining the three months (June-August) in the `interval_of_interest()` function. Then we define a function `get_request()` which will build request to the Sentinel Hub API on the Copernicus Dataspace Ecosystem.\n",
    "\n",
    "In this [`SentinelHubRequest`](https://sentinelhub-py.readthedocs.io/en/latest/reference/sentinelhub.api.process.html#sentinelhub.api.process.SentinelHubRequest) we are defining the input data, time-frame, the output type (TIFF), the bounding box, resolution and where to save the data.\n",
    "\n",
    "We define this as a function because we want to make multiple requests where only the year changes, so having this as a function where the only input is the year makes this more straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interval_of_interest(year):\n",
    "    return (datetime(year, 6, 1), datetime(year, 9, 1))\n",
    "\n",
    "\n",
    "def get_request(year):\n",
    "    time_interval = interval_of_interest(year)\n",
    "    return SentinelHubRequest(\n",
    "        evalscript=evalscript_cloudless,\n",
    "        input_data=[\n",
    "            SentinelHubRequest.input_data(\n",
    "                data_collection=DataCollection.SENTINEL2_L2A.define_from(\n",
    "                    \"s2\", service_url=config.sh_base_url\n",
    "                ),\n",
    "                time_interval=time_interval,\n",
    "            )\n",
    "        ],\n",
    "        responses=[SentinelHubRequest.output_response(\"default\", MimeType.TIFF)],\n",
    "        bbox=bbox,\n",
    "        resolution=resolution,\n",
    "        config=config,\n",
    "        data_folder=\"./results\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell now creates a requests for each of the years, from 2018 to 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of requests\n",
    "requests = {}\n",
    "for year in range(2018, 2024):\n",
    "    requests[year] = get_request(year)\n",
    "\n",
    "requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to download the data. This is done with the utility function `SentinelHubDownloadClient`. It downloads a list of requests in parallel making the download much faster. Before we can do that, we need to change the format of the requests slightly which is done in the variable `list_of_requests`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_requests = [request.download_list[0] for request in requests.values()]\n",
    "\n",
    "# download data with multiple threads\n",
    "data = SentinelHubDownloadClient(config=config).download(\n",
    "    list_of_requests, max_threads=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the requests does not provide any information from which year the data is, so we rename the output of each request to the year of the data it represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_output_path(request):\n",
    "    # Gets the full path to the output from a request\n",
    "    return Path(request.data_folder, request.get_filename_list()[0])\n",
    "\n",
    "\n",
    "# Moves and renames the files to the root directory of results\n",
    "for year, request in requests.items():\n",
    "    request_output_path(request).rename(f\"./results/{year}.tif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data with xarray\n",
    "\n",
    "Now we can load the data into [xarray](https://docs.xarray.dev/en/stable/). We use [rioxarray](https://corteva.github.io/rioxarray/html/index.html), an extension for xarray to load multiple Tiffs into a single xarray Dataset.\n",
    "xarray is a scalable tool to analyse multi-dimensional data in Python. Because of this xarray is ideally suited to analyse time series data.\n",
    "\n",
    "The different files correspond to the time-dimension, however xarray does not know which file is which time step. Because of this we add a pre-processing step where we parse out the year from the filename and add it as the time dimension for that file.\n",
    "\n",
    "The errors in the output can be safely ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_dim(xda):\n",
    "    # This pre-processes the file to add the correct\n",
    "    # year from the filename as the time dimension\n",
    "    year = int(Path(xda.encoding[\"source\"]).stem)\n",
    "    return xda.expand_dims(year=[year])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiff_paths = Path(\"./results\").glob(\"*.tif\")\n",
    "ds_s2 = xr.open_mfdataset(\n",
    "    tiff_paths, engine=\"rasterio\", preprocess=add_time_dim, band_as_variable=True\n",
    ")\n",
    "ds_s2 = ds_s2.rename(\n",
    "    {\n",
    "        \"band_1\": \"R\",\n",
    "        \"band_2\": \"G\",\n",
    "        \"band_3\": \"B\",\n",
    "        \"band_4\": \"NDVI\",\n",
    "    }\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results in a xarray dataset with 3 coordinates year, x and y as well as the data variables returned by the evalscript as data variables in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_s2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use xarray to plot the RGB data as a true color image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get RGB data for a year\n",
    "true_color = ds_s2.sel(year=2018)[[\"R\", \"G\", \"B\"]].to_array()\n",
    "# Divide by scale factor and apply gamma to brighten image\n",
    "(true_color / 10000 * 4).plot.imshow()\n",
    "plt.title(\"True Color\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also similarly plot the NDVI values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_s2.NDVI.plot(cmap=\"PRGn\", x=\"x\", y=\"y\", col=\"year\", col_wrap=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "For analysis the first step is to classify pixels as forest. In our case we will just do a simple thresholding classification where we classify everything above a certain threshold as forest. This isn't the best approach for classifying forest, since agricultural areas can also easily reach very high NDVI values. A better approach would be to classify based on the temporal signature of the pixel. \n",
    "\n",
    "However for this basic analysis we stick to the simple thresholding approach.\n",
    "\n",
    "In this case we classify everything above an NDVI of 0.7 as forest. This calculated Forest mask is then saved to a new Data Variable in the xarray DataSet: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_s2[\"FOREST\"] = ds_s2.NDVI > (0.7 * 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this forest mask we can already do a quick preliminary analysis to plot the total forest area over the years.\n",
    "\n",
    "To do this we sum up the pixels along the x and y coordinate but not along the time coordinate. This will leave us with one value per year representing the number of pixels classified as forest. With the resolution of a pixel we can then calculate the forest area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_km2(dataarray, resolution):\n",
    "    # Calculate forest area\n",
    "    return dataarray * np.prod(list(resolution)) / 1e6\n",
    "\n",
    "\n",
    "forest_pixels = ds_s2.FOREST.sum([\"x\", \"y\"])\n",
    "forest_area_km2 = to_km2(forest_pixels, resolution)\n",
    "forest_area_km2.plot()\n",
    "plt.title(\"Forest Cover\")\n",
    "plt.ylabel(\"Forest Cover [km²]\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the total forest area decreased in this area from around 80 km² in 2018 to only around 50 km² in 2023.\n",
    "\n",
    "The next steps is to make change maps from year to year. To do this we basically take the difference of the forest mask of one year and its previous year.\n",
    "\n",
    "This will result in 0 values where there has been no change, -1 where forest was lost and +1 where forest was gained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make change maps of forest loss and forest gain compared to previous year\n",
    "\n",
    "# 0 - 0 = No Change: 0\n",
    "# 1 - 1 = No Change: 0\n",
    "# 0 - 1 = Forest Gain: 1\n",
    "# 1 - 0 = Forest Loss: -1\n",
    "\n",
    "# Define custom colors and labels\n",
    "colors = [\"darkred\", \"white\", \"darkblue\"]\n",
    "labels = [\"Forest Loss\", \"No Change\", \"Forest Gain\"]\n",
    "\n",
    "# Create a colormap and normalize it\n",
    "cmap = mcolors.ListedColormap(colors)\n",
    "norm = plt.Normalize(-1, 1)  # Adjust the range based on your data\n",
    "\n",
    "ds_s2[\"CHANGE\"] = ds_s2.FOREST.astype(int).diff(\"year\", label=\"upper\")\n",
    "ds_s2.CHANGE.sel(year=2022).plot(cmap=cmap, norm=norm, add_colorbar=False)\n",
    "\n",
    "# Create a legend with string labels\n",
    "legend_patches = [\n",
    "    mpatches.Patch(color=color, label=label) for color, label in zip(colors, labels)\n",
    "]\n",
    "plt.legend(handles=legend_patches, loc=\"lower left\")\n",
    "plt.title(\"Forest Change Map\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the spatial distribution of areas affected by forest loss. In the displayed change from 2021 to 2022 most of the loss happened in the northern part of the study area, while the southern part had comparatively less lost area.\n",
    "\n",
    "To get a feel for the loss per year we can sum up the lost areas per year. This should follow basically the same trends as the earlier plot of total forest area. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forest Loss per Year\n",
    "forest_loss = (ds_s2.CHANGE == -1).sum([\"x\", \"y\"])\n",
    "forest_loss_km2 = to_km2(forest_loss, resolution)\n",
    "forest_loss_km2.plot()\n",
    "plt.title(\"Forest Loss per Year\")\n",
    "plt.ylabel(\"Forest Loss [km²]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there have been two years with particularly large amounts of lost forest area. From 2019-2020 and with by far the most lost area between 2021 and 2022."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "Finally we want to see how accurate our data is compared to the widely used Hansen Global Forest Change data. In a real scientific scenario we would use Ground-Truth data to assess the accuracy of our classification. In this case we use the Global Forest Change data in place of Ground Truth data, just to show how an accuracy assessment can be done. The assessment we are doing only shows how accurately we replicate the Global Forest Change data, however we will not know if our product is more or less accurate. For this assessment actual Ground Truth data would have to be used.\n",
    "\n",
    "First we download the Global Forest Change Data [here](https://storage.googleapis.com/earthenginepartners-hansen/GFC-2022-v1.10/download.html) and open it using xarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file\n",
    "ground_truth = (\n",
    "    xr.open_dataarray(\n",
    "        \"./data/Hansen_GFC-2022-v1.10_lossyear_60N_010E.tif\", engine=\"rasterio\"\n",
    "    )\n",
    "    .rio.clip_box(*bbox_coords)\n",
    "    .rio.reproject(epsg)\n",
    "    .sel(band=1)\n",
    "    .where(lambda gt: gt < 100, 0)  # fill no-data (values over 100) with 0\n",
    ")\n",
    "ground_truth.plot(levels=range(24), cbar_kwargs={\"label\": \"Year of Forest Loss\"})\n",
    "plt.title(\"Global Forest Watch Data\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data shows in which year forest was first lost. To compare with our own data we need to add the data to our DataSet. To do this the data needs to have the same coordinates. This can be achieved with `.interp_like()`. This function interpolates the data to match up the coordinates of another DataSet.\n",
    "\n",
    "In this case we chose the interpolation method `nearest` since it is categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_s2[\"GROUND_TRUTH\"] = ground_truth.interp_like(ds_s2, method=\"nearest\").astype(int)\n",
    "ds_s2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ground truth data saves the year when deforestation was first detected for a pixel in a single raster. To do this, it encodes the year of forest loss as an integer, giving the year. So an integer of 21 means the pixel was first detected as deforested in 2021, whereas a value of 0 means that deforestation was never detected.\n",
    "\n",
    "Currently our classification saves the deforestation detection in multiple rasters, one for each year. To get our data into a format that is similar to our comparison data we need to convert our rasters for each time step into a single one.\n",
    "\n",
    "To do this we first assign all pixels which were detected as deforestation (`CHANGE == -1`) to the year in which the deforestation was detected (`lost_year`). Then we compute over our time-series the first occurence of deforestation (equivalent to the first non-zero value) per pixel. This is then saved in a new data variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert lost forest (-1) into the year it was lost\n",
    "lost_year = (ds_s2.CHANGE == -1) * ds_s2.year % 100\n",
    "first_nonzero = (lost_year != 0).argmax(axis=0).compute()\n",
    "ds_s2[\"LOST_YEAR\"] = lost_year[first_nonzero]\n",
    "ds_s2.LOST_YEAR.plot(cbar_kwargs={\"label\": \"Year of Forest Loss\"})\n",
    "plt.title(\"Classification Forest Loss Year\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing this visually to the Global Forest Watch data, allows us to do some initial quality assessment. We can see definite differences between the two datasets. The Global Forest Watch data has much more clearly defined borders. In general our classification seems to overestimate deforestation. However the general pattern of forest loss is the same in both. Most of the deforestation is in the north of the study area, with less forest loss in the south.\n",
    "\n",
    "There are a few reasons for those differences. The main differences has to be in our much more simple approach to forest classification and change detection. It is expected that our approach will lead to large amounts of commision errors since changes are only confirmed using a single observation. It however can also lead to a lot of omission errors since the NDVI thresholding might classify highly productive non-forest areas as forest due to their high NDVI values. \n",
    "\n",
    "However there are also some systematic differences. Our algorithm looks at differences between the middle of the years, which means that some changes can happen at the end of the growing year which will be detected first in the next year whereas the Global Forest Watch dataset will detect it in the correct (earlier) year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_s2.GROUND_TRUTH.plot(cbar_kwargs={\"label\": \"Year of Forest Loss\"})\n",
    "plt.title(\"Global Forest Watch - Interpolated\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can also calculate an accuracy score. This is a score from 0-1, where values close to 0.5 basically mean that the classification is random and values close to 1 mean that most of the values of our comparison data and classification data match. \n",
    "\n",
    "First we look at the overall accuracy of forest loss over the entire time period from 2018 to 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = accuracy_score(\n",
    "    (ds_s2.LOST_YEAR > 18).values.ravel(), (ds_s2.GROUND_TRUTH > 18).values.ravel()\n",
    ")\n",
    "print(f\"The overall accuracy of forest loss detection is {score:.2f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected from the visual interpretation, with an accuracy of 0.77, our product differs quite a lot compared to the Global Forest Watch data. From this we do not know for sure that our product is less accurate compared to the actual forest loss patterns observed on the ground. We only know that it is different to the Global Forest Watch product. It might be more accurate or less accurate. \n",
    "\n",
    "However because of the simplicity of our algorithm it is safe to assume that our output is less accurate. \n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook showed how to efficiently access data stored on the Copernicus Dataspace Ecosystem using the Sentinel Hub APIs. This includes generating cloud free mosaics and calculating spectral indices in the cloud. \n",
    "\n",
    "It also showed how to import this data using xarray and carry out a basic multi-temporal detection of forest loss.\n",
    "\n",
    "This notebook should serve as a starting point for your own analysis using the powerful Python Data Analysis ecosystem and leveraging the Copernicus Dataspace Ecosystem APIs for quick satellite data access."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring NO2 concentrations\n",
    "\n",
    "Nitrogen dioxide is primarily produced by the burning of fossil fuels. High concentration of Nitrogen dioxide are widely known to cause inlammation of the airways, with long-term exposure leading to respiratory problems such as asthma and bronchitis. Areas with high-density road networks close to large populations are most at risk of over-exposure. For thise with pre-existing conditions, along with children and the elderly, this risk is more pronounced. Therefore, regular monitoring of Nitrogen dioxide concentrations is essential for avoiding health risks and keepint track of pollution.\n",
    "The main objective of the Copernicus Sentinel-5P mission is to perform atmospheric measurements with high spatio-temporal resolution, to be used for air quality, ozone & UV radiation, and climate monitoring & forecasting.\n",
    "\n",
    "Sources: https://www.aeroqual.com/measurements/nitrogen-dioxide, \n",
    "https://sentinels.copernicus.eu/web/sentinel/missions/sentinel-5p\n",
    "\n",
    "The Copernicus Browser and the connected APIs offer you new and more efficient ways to access and analyse data.\n",
    "\n",
    "## Outline\n",
    "\n",
    "This notebook analyses air pollution in Europe using the TROPOMI sensor on the Sentinel 5P satellite. This notebook aims to provide data to answer the following questions:\n",
    "\n",
    "- What is the spatial distribution of NO2 concentration in Europe\n",
    "- How does the NO2 concentration vary over a year\n",
    "- Which European capitals are most affected by NO2 emissions\n",
    "\n",
    "## Used tools and features\n",
    "\n",
    "To carry out these analyses we will cover a few different concepts and features available on the Copernicus Dataspace Ecosystem:\n",
    "\n",
    "- Downloading of Raw data using custom resolutions and bounding boxes\n",
    "- Calculation of monthly mosaics on the fly in the cloud\n",
    "- Direct access to timeseries data for geometries through the statistical API\n",
    "- The popular [pandas](https://pandas.pydata.org/) Python library to analyse tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import getpass\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import rasterio.plot\n",
    "import seaborn as sns\n",
    "from rasterio import features\n",
    "from sentinelhub import (\n",
    "    CRS,\n",
    "    BBox,\n",
    "    DataCollection,\n",
    "    MimeType,\n",
    "    SentinelHubRequest,\n",
    "    SentinelHubStatistical,\n",
    "    SentinelHubStatisticalDownloadClient,\n",
    "    SHConfig,\n",
    "    parse_time,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Credentials\n",
    "\n",
    "To obtain your `client_id` & `client_secret` you need to navigate to your [Dashboard](https://shapps.dataspace.copernicus.eu/dashboard/#/). In the User Settings you can create a new OAuth Client to generate these credentials. For more detailed instructions, visit the relevent [documentation page](https://documentation.dataspace.copernicus.eu/APIs/SentinelHub/Overview/Authentication.html).\n",
    "\n",
    "Now that you have your `client_id` & `client_secret`, it is recommended to configure a new profile in your Sentinel Hub Python package. Instructions on how to configure your Sentinel Hub Python package can be found [here](https://sentinelhub-py.readthedocs.io/en/latest/configure.html). Using these instructions you can create a profile specific to using the package for accessing Copernicus Data Space Ecosystem data collections. This is useful as changes to the the config class are usually only temporary in your notebook and by saving the configuration to your profile you won't need to generate new credentials or overwrite/change the default profile each time you rerun or write a new Jupyter Notebook. \n",
    "\n",
    "If you are a first time user of the Sentinel Hub Python package for Copernicus Data Space Ecosystem, you should create a profile specific to the Copernicus Data Space Ecosystem. You can do this in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Only run this cell if you have not created a configuration.\n",
    "\n",
    "config = SHConfig()\n",
    "config.sh_client_id = getpass.getpass(\"Enter your SentinelHub client id\")\n",
    "config.sh_client_secret = getpass.getpass(\"Enter your SentinelHub client secret\")\n",
    "config.sh_token_url = \"https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token\"\n",
    "config.sh_base_url = \"https://sh.dataspace.copernicus.eu\"\n",
    "config.save(\"cdse\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if you have already configured a profile in Sentinel Hub Python for the Copernicus Data Space Ecosystem, then you can run the below cell entering the profile name as a string replacing `profile_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SHConfig(\"profile_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing Spatial Distribution\n",
    "\n",
    "Let's first get an overview of our study area, which is most of mainland Europe. To get this overview we first define an evalscript. An evalscript is a piece of javascript code which specifies how each pixel should be handled. For the first one we just define the input band that we want to look at, which is `NO2` and return that band immediately, without carrying out any more calculations before the data is returned to us.\n",
    "\n",
    "For more information on evalscripts have a look at the [documentation](https://documentation.dataspace.copernicus.eu/APIs/SentinelHub/Evalscript.html).\n",
    "\n",
    "```js\n",
    "//VERSION=3\n",
    "function setup() {\n",
    "   return {\n",
    "    input: [\"NO2\"], // This specifies the bands that are looked at\n",
    "    output: { \n",
    "      bands: 1,\n",
    "      // This specifies in which data type the values will be returned\n",
    "      sampleType: \"FLOAT32\"\n",
    "    },\n",
    "    // Will make a simple mosaic, taking the most recent tiles to fill the bounding box\n",
    "    mosaicking: \"SIMPLE\"\n",
    "  };\n",
    "}\n",
    "\n",
    "function evaluatePixel(samples) {\n",
    "    // Here we could do more calculations which are applied to each pixel, \n",
    "    // but for now let's just return the value \n",
    "   return [samples.NO2] \n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also need to define the evalscript as a Python variable\n",
    "evalscript_raw = \"\"\"\n",
    "//VERSION=3\n",
    "function setup() {\n",
    "   return {\n",
    "    input: [\"NO2\"], // This specifies the bands that are looked at\n",
    "    output: {\n",
    "      bands: 1,\n",
    "      // This specifies in which data type the values will be returned\n",
    "      sampleType: \"FLOAT32\"\n",
    "    }\n",
    "  };\n",
    "}\n",
    "\n",
    "function evaluatePixel(samples) {\n",
    "    // Here we could do more calculations which are applied to each pixel,\n",
    "    // but for now let's just return the value\n",
    "   return [samples.NO2]\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the evalscript we can now make a request for data.\n",
    "\n",
    "The request will take care of a lot of things for us. It will return our the data in our specified resolution and bounding box, for our specified time range and it will automatically mosaic multiple tiles together to fill the entire bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_europe = BBox([-12.30, 34.59, 32.52, 63.15], crs=CRS.WGS84).transform(CRS(3857))\n",
    "# This is defining the data we will use.\n",
    "# You can list all available data collections with `DataCollection.get_available_collections()`.\n",
    "data_5p = DataCollection.SENTINEL5P.define_from(\"5p\", service_url=config.sh_base_url)\n",
    "\n",
    "request_raw = SentinelHubRequest(\n",
    "    evalscript=evalscript_raw,\n",
    "    input_data=[\n",
    "        SentinelHubRequest.input_data(\n",
    "            data_collection=data_5p,\n",
    "            time_interval=(\"2023-01-01\", \"2023-05-26\"),\n",
    "        )\n",
    "    ],\n",
    "    responses=[SentinelHubRequest.output_response(\"default\", MimeType.TIFF)],\n",
    "    bbox=bbox_europe,\n",
    "    # Resolution is defined in units of the bbox crs! Be careful with WGS84 since this will be in degrees!\n",
    "    # Since we have defined our bounding box in Web mercator the resolution is in meters.\n",
    "    resolution=(5500, 3500),\n",
    "    config=config,\n",
    "    data_folder=\"./data\",  # We save the data in a specified folder\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've defined the request, we can get the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = request_raw.get_data(save_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a function which plots the data of the request together with the borders of the European countries, taken from the natural earth dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = (\n",
    "    gpd.read_file(\"./data/ne_50m_admin_0_countries/ne_50m_admin_0_countries.shp\")\n",
    "    .to_crs(3857)\n",
    "    .cx[bbox_europe.min_x : bbox_europe.max_x, bbox_europe.min_y : bbox_europe.max_y]\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "countries = countries[[\"ADMIN\", \"geometry\"]]\n",
    "\n",
    "\n",
    "def plot_request(request, bbox, title=\"\"):\n",
    "    image_path = Path(request.data_folder) / request.get_filename_list()[0]\n",
    "    with rasterio.open(image_path) as raster:\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        ax.set_xlim([bbox.min_x, bbox.max_x])\n",
    "        ax.set_ylim([bbox.min_y, bbox.max_y])\n",
    "        ax.set_title(title)\n",
    "        rasterio.plot.show(raster, ax=ax)\n",
    "        countries.plot(ax=ax, facecolor=\"none\", edgecolor=\"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_request(request_raw, bbox_europe, \"NO2 concentrations for a single day\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that even though the data is already mosaiced together, filling the entire bounding box, we do not have data everywhere since Tropomi does have a bunch of missing data per acquisition depending on atmospheric conditions. \n",
    "\n",
    "With this image we can already see some patterns but let's try to get a more representative image and take the mean of the NO2 values over an entire month to get a more complete picture.\n",
    "\n",
    "To do this we do not have to download all of the data for an entire month, instead we can extend our evalscript so that the mean value for a month is calculated in the cloud for us. Doing it like this saves us a bunch of time downloading all images. So let's have a look at the updated evalscript:\n",
    "\n",
    "The most important thing that changed is that we now changed the mosaicking input to ORBIT. This gives us all acquisitions for a time series to calculate values from. In the input we also have added `dataMask` as a band. This will tell us, if the NO2 band has data or not. We are using this to remove acquisitions without data from our calculation.\n",
    "\n",
    "In our `evaluatePixel` function we have added two more steps. The first one is to filter out all acquisitions which do not have data with the `isClear()` function. After we have filtered the time series we can calculate the mean of all values using the `sum()` function and the length of the clear timeseries.\n",
    "\n",
    "In the end we return the mean value we have calculated.\n",
    "\n",
    "```js\n",
    "//VERSION=3\n",
    "function setup() {\n",
    "    return {\n",
    "        input: [\"NO2\", \"dataMask\"],\n",
    "        output: {\n",
    "            bands: 1,\n",
    "            sampleType: \"FLOAT32\",\n",
    "        },\n",
    "        mosaicking: \"ORBIT\"\n",
    "    };\n",
    "}\n",
    "\n",
    "function isClear(sample) {\n",
    "    return sample.dataMask == 1;\n",
    "}\n",
    "\n",
    "function sum(array) {\n",
    "    let sum = 0;\n",
    "    for (let i = 0; i < array.length; i++) {\n",
    "        sum += array[i].NO2;\n",
    "    }\n",
    "    return sum;\n",
    "}\n",
    "\n",
    "function evaluatePixel(samples) {\n",
    "    const clearTs = samples.filter(isClear)\n",
    "    const mean = sum(clearTs) / clearTs.length\n",
    "    return [mean]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalscript_mean_mosaic = \"\"\"\n",
    "//VERSION=3\n",
    "function setup() {\n",
    "    return {\n",
    "        input: [\"NO2\", \"dataMask\"],\n",
    "        output: {\n",
    "            bands: 1,\n",
    "            sampleType: \"FLOAT32\",\n",
    "        },\n",
    "        mosaicking: \"ORBIT\"\n",
    "    };\n",
    "}\n",
    "\n",
    "function isClear(sample) {\n",
    "    return sample.dataMask == 1;\n",
    "}\n",
    "\n",
    "function sum(array) {\n",
    "    let sum = 0;\n",
    "    for (let i = 0; i < array.length; i++) {\n",
    "        sum += array[i].NO2;\n",
    "    }\n",
    "    return sum;\n",
    "}\n",
    "\n",
    "function evaluatePixel(samples) {\n",
    "    const clearTs = samples.filter(isClear)\n",
    "    const mean = sum(clearTs) / clearTs.length\n",
    "    return [mean]\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_monthly = SentinelHubRequest(\n",
    "    evalscript=evalscript_mean_mosaic,\n",
    "    input_data=[\n",
    "        SentinelHubRequest.input_data(\n",
    "            data_collection=data_5p,\n",
    "            time_interval=(\"2022-12-01\", \"2023-01-01\"),\n",
    "        )\n",
    "    ],\n",
    "    responses=[SentinelHubRequest.output_response(\"default\", MimeType.TIFF)],\n",
    "    bbox=bbox_europe,\n",
    "    resolution=(5000, 3500),\n",
    "    config=config,\n",
    "    data_folder=\"./data\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now made a request for an entire month of data, for December of 2022. Other than that nothing much changed in the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_data = request_monthly.get_data(save_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_request(request_monthly, bbox_europe, \"Mean NO2 concentrations December 2022\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks much better, since it is Winter the northern latitudes and the alps still do not have much data due to snow cover and low sun angles, however most of the rest of Europe is now covered by data.\n",
    "\n",
    "We can clearly see NO2 hot spots around developed areas, like the Po Valley in Italy and the Ruhr area in Germany. You can also clearly make out the effect of some cities, like Istanbul and Madrid.\n",
    "\n",
    "Doing it the traditional way, you would have to download at least 18GB of data to be able to have all the data necessary to start computing the mosaic. In our case we just need to download the finished mosaic for our area of interest which has a size of 1.3 MB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try it yourself!\n",
    "\n",
    "You can try creating a mosaic for a different month. For example take a look a NO2 concentrations in April 2020 where the Covid lockdowns happened. \n",
    "\n",
    "You might also want to see how different time frames look like. Try with just a week or extend it to 3 months or even a year. But be aware that mosaics over a long time frame are a very good way to burn through your free included Processing Units!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing European countries\n",
    "\n",
    "Now let's look at the distribution of values per country to see which countries had the highest average NO2 values in the month. \n",
    "\n",
    "To do this we rasterize all countries in our area of interest. We do this so that we can select all array values which are covered by a certain country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries[\"ID\"] = countries.index\n",
    "\n",
    "image_path = Path(request_monthly.data_folder) / request_monthly.get_filename_list()[0]\n",
    "with rasterio.open(image_path) as src:\n",
    "    affine = src.transform\n",
    "# convert gpd Dataframe to format accepted by rasterize\n",
    "geo_iter = list(countries[[\"geometry\", \"ID\"]].itertuples(index=False, name=None))\n",
    "# This call is converting the array into a raster with the same size as our NO2 raster\n",
    "country_array = features.rasterize(\n",
    "    geo_iter, transform=affine, out_shape=mean_data[0].shape, fill=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define two helper functions which get all NO2 values in a country and another function which calcuates the mean of those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_array(country_id):\n",
    "    return mean_data[0][country_array == country_id]\n",
    "\n",
    "\n",
    "def get_mean(country_id):\n",
    "    return np.nanmean(get_array(country_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is then applied to the countries dataframe, to fill a new column `mean` which holds the mean NO2 values per country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries[\"mean\"] = countries.apply(lambda x: get_mean(x[\"ID\"]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then sort by that mean value and have a look at the countries with the highest mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = countries.sort_values(\"mean\", ascending=False)\n",
    "sorted_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the 5 countries with the highest mean we are then plotting a boxplot of NO2 values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the country ids with the 5 highest mean values\n",
    "n_countries = 5\n",
    "country_ids = list(sorted_df[\"ID\"][:n_countries])\n",
    "country_names = list(sorted_df[\"ADMIN\"][:n_countries])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(data=[get_array(country_id) for country_id in country_ids])\n",
    "ax.set_xticklabels(country_names);\n",
    "ax.set_title(\"Distribution of NO2 values in countries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can see that even though the mean of values in Germany is the 4th lowest, of the 5 countries it has the absolute highest values. We can also see that Belgium and Germany both have qutie the large variance in NO2 values, with some areas of low NO2 concentration and some areas quite high concentrations.\n",
    "\n",
    "### Analysing EU Capitals\n",
    "\n",
    "Now we want to take a look at EU capitals specifically. For this more focused analysis we want to analyse time series data. To do this we are taking advantage of another API capability, the [Statistical API](https://documentation.dataspace.copernicus.eu/APIs/SentinelHub/Statistical.html).\n",
    "\n",
    "Even for the previous analysis of European countries, if we were not interested at all in the spatial distribution of data and only interested in statistics for certain geometries, the statistical API would have been the perfect fit. It removes the need to download a lot of data to calculate statistics for areas. Instead it does all of the calculation of statistics like mean, max, min and standard deviation in the cloud and in the end only sends those values. \n",
    "\n",
    "Another capability of the API is the easy chunking in regular intervals, which we will be using to make the time series.\n",
    "\n",
    "But let's first import the EU capitals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load capitals\n",
    "capitals = gpd.read_file(\"./data/eu_capitals.geojson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try it yourself!\n",
    "\n",
    "You can also add a point which you are interested in so that we can later compare it to the European capitals. We have selected Faro in Portugal, which is supposed to be one of the cities in the EU with the [cleanest air](https://www.eea.europa.eu/themes/air/urban-air-quality/european-city-air-quality-viewer).\n",
    "\n",
    "You can find the lat lon coordinates of your point of interest for example through this [website](https://www.latlong.net/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = 38.02\n",
    "lon = -7.93\n",
    "custom_point = \"Faro\"\n",
    "\n",
    "extra_point = gpd.GeoDataFrame(\n",
    "    [custom_point],\n",
    "    columns=[\"name\"],\n",
    "    geometry=gpd.points_from_xy([lat], [lon], crs=4326),\n",
    ")\n",
    "capitals = pd.concat([capitals, extra_point])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Process API examples, we have seen how to obtain satellite imagery. Statistical API can be used in a very similar way. The main difference is that the results of Statistical API are aggregated statistical values of satellite data instead of entire images. In many use cases, such values are all that we need. By using Statistical API we can avoid downloading and processing large amounts of satellite data.\n",
    "\n",
    "All general rules for building evalscripts apply. However, there are some specifics when using evalscripts with the Statistical API:\n",
    "\n",
    "- The `evaluatePixel()` function must, in addition to other output, always return a `dataMask` output. This output defines which pixels are excluded from calculations. For more details and an example, see [here](https://documentation.dataspace.copernicus.eu/APIs/SentinelHub/Statistical.html).\n",
    "- The default value of sampleType is `FLOAT32`.\n",
    "- The output.bands parameter in the setup() function can be an array. This makes it possible to specify custom names for the output bands and different output `dataMask` for different outputs, see this [example](https://documentation.dataspace.copernicus.eu/APIs/SentinelHub/Statistical/Examples.html#multiple-outputs-with-different-datamasks-multi-band-output-with-custom-bands-names-and-different-histogram-types)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalscript_stat = \"\"\"\n",
    "//VERSION=3\n",
    "function setup() {\n",
    "    return {\n",
    "        input: [\"NO2\", \"dataMask\"],\n",
    "        output: [{\n",
    "          id: \"default\",\n",
    "          bands: [\"NO2\"],\n",
    "          sampleType: \"FLOAT32\"\n",
    "        },\n",
    "        {\n",
    "          id: \"dataMask\",\n",
    "          bands: 1,\n",
    "        }]\n",
    "    };\n",
    "}\n",
    "\n",
    "function evaluatePixel(samples) {\n",
    "    const statsVal = isFinite(samples.NO2) ? samples.NO2 : NaN; \n",
    "    return {default: [statsVal], dataMask: [samples.dataMask]}\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the Statistical API request, for that we first define an aggregation. Here we define the time range we are interesed in. In this case it is one year of data, all of 2022. We then define the aggregation interval, this defines how many days are aggregated. Since Sentinel 5P has a very high revisit rate we can define a temporal resolution of one day. However we could just as easily make a time series of weekly or monthly values just by changing the aggregation interval to `P1W` or `P1M` respectively.\n",
    "\n",
    "The size is set to 1 by 1 pixel since Sentinel 5P pixels are quite large and we are only intersted in point data for the capitsls.\n",
    "\n",
    "### Try it yourself!\n",
    "\n",
    "You can also try selecting a different aggregation interval, like 5 days or 1 week and see how the output changes. Be aware though that the current evalscript will not return a mean of all values in the aggregation interval, instead it will only return the most recent value in the time series. You could also try adapting the mosaic evalscript from earlier to get it to return the mean value for each aggregation interval (basically you need to add the `dataMask` output which is missing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregation = SentinelHubStatistical.aggregation(\n",
    "    evalscript=evalscript_stat,\n",
    "    time_interval=(\"2022-01-01\", \"2023-01-01\"),\n",
    "    aggregation_interval=\"P2D\",\n",
    "    size=(1,1),\n",
    ")\n",
    "\n",
    "input_data = SentinelHubStatistical.input_data(\n",
    "    DataCollection.SENTINEL5P.define_from(\"5p\", service_url=config.sh_base_url)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create one request for each capital city. Instead of doing it like this we could also use the [Batch Statistical API](https://documentation.dataspace.copernicus.eu/APIs/SentinelHub/BatchStatistical.html) which is designed to calculate statistics for many polygons efficiently. \n",
    "\n",
    "This list of requests is then downloaded in parallel.\n",
    "\n",
    "To speed running this cell up a bit and to reduce the amount of Processing Units spent by running this notebook, we are only selecting the capitals of the four countries with the highest NO2 concentrations in the first analysis. This will show us if the high concentration also apply to the capital of that country and if the high concentrations are present all throughout the year.\n",
    "\n",
    "It is however definitely possible to also run the next cell for all capitals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capitals.geometry = capitals.to_crs(3857).buffer(2000)\n",
    "to_get = [\"Amsterdam\", \"Brussels\", \"Tallinn\", \"Berlin\", custom_point]\n",
    "selected_capitals = capitals.loc[capitals[\"name\"].isin(to_get)]\n",
    "\n",
    "requests = []\n",
    "\n",
    "for geo_shape in selected_capitals.geometry.values:\n",
    "    request = SentinelHubStatistical(\n",
    "        aggregation=aggregation,\n",
    "        input_data=[input_data],\n",
    "        config=config,\n",
    "        bbox=BBox(geo_shape.bounds, crs=selected_capitals.crs)\n",
    "    )\n",
    "    requests.append(request)\n",
    "\n",
    "download_requests = [request.download_list[0] for request in requests]\n",
    "client = SentinelHubStatisticalDownloadClient(config=config)\n",
    "pollution_stats = client.download(download_requests, max_threads=5, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You get back a json response with all the statistical values for the area. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pollution_stats[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the structure of the json response it is hard to do any analysis directly with the response. Because of that we want to transform that data to a pandas dataframe. In the next cell we build the dataframe from the request output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no2_dfs = [pd.json_normalize(stats[\"data\"]) for stats in pollution_stats]\n",
    "# Add name of the capital to the corresponding data\n",
    "for df, capital in zip(no2_dfs, selected_capitals.name):\n",
    "    df[\"name\"] = capital\n",
    "# Build one dataframe out of the many seperate ones\n",
    "no2_df = pd.concat(no2_dfs, ignore_index=True)\n",
    "# Simplify the column names\n",
    "stat_cols = [col for col in no2_df.columns if 'stats' in col]\n",
    "# Convert to numeric\n",
    "no2_df[stat_cols] = no2_df[stat_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "no2_df = no2_df.rename(columns=lambda x: x.split(\".\")[-1])\n",
    "no2_df[\"month\"] = no2_df[\"from\"].astype(\"datetime64[ns, UTC]\").dt.month\n",
    "no2_df.to_csv(\"./data/no2_capitals_timeseries.csv\")\n",
    "no2_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this dataframe we can now do analysis. To give an example we are looking at the time series for a few different capital cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no2_df = pd.read_csv(\"./data/no2_capitals_timeseries.csv\")\n",
    "\n",
    "sns.lineplot(\n",
    "    data=no2_df,\n",
    "    x=\"month\",\n",
    "    y=\"mean\",\n",
    "    hue=\"name\",\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows us that for most cities the NO2 concentration is seasonal, with higher values in winter than in Summer. Amsterdam, Berlin and Brussels all share quite similar temporal patterns. Tallinn on the other hand has much lower NO2 concentration throughout.\n",
    "\n",
    "## Summary\n",
    "\n",
    "With this data now acquired many different types of analysis can be carried out. This showed the advantage of statistical API, since you don't have to download entire tiles even if you are only interested in the value of a single pixel. This makes data access much more efficient, allowing you to get started with analysis much quicker.\n",
    "\n",
    "For next steps you can have a look at the [batch statistical API](https://documentation.dataspace.copernicus.eu/APIs/SentinelHub/BatchStatistical.html) which allows you to do statstical analysis efficiently at a much larger scale."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
